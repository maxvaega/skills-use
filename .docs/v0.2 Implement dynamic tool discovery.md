# v0.2 Feature: Dynamic Tool Discovery from Scripts

**Status**: Planning
**Priority**: High (deferred from v0.1)
**Target Release**: v0.2
**Estimated Effort**: 15-20 hours
**Created**: 2025-11-03

---

## Overview

### Feature Description

Enable skills to dynamically declare and expose executable scripts (Python, JavaScript, Shell) as LangChain tools that the agent can discover and invoke during skill execution. This implements the "progressive disclosure" pattern at the tool level: skills load metadata → content → **additional tools**.

### Current Behavior (v0.1)

When a skill is invoked:
1. Skill metadata is loaded (name, description, allowed-tools)
2. Full SKILL.md content is loaded and processed
3. Base directory is injected
4. Arguments are substituted
5. Processed content is returned to the agent as a string

**Limitation**: If the skill directory contains executable scripts in `scripts/`, the agent only learns about them by reading the skill content. The agent must manually decide to execute scripts—they are not presented as first-class tools.

### Desired Behavior (v0.2)

When a skill is invoked:
1. All steps from v0.1 continue to work
2. **Additionally**: The library scans the skill's `scripts/` directory
3. For each discovered script, a new LangChain `StructuredTool` is created
4. These tools are returned alongside the skill content
5. The agent can invoke these tools directly without manual script execution

**Example**: A skill `pdf-processor` might expose:
- Tool: `extract_text_from_pdf` (script: `scripts/extract_text.py`)
- Tool: `compress_pdf` (script: `scripts/compress.py`)
- Tool: `merge_pdfs` (script: `scripts/merge.py`)

---

## Rationale for Deferment from v0.1

### Why This Was Removed from v0.1 MVP

During MVP validation using the `mvp-validator` skill, this feature was identified as a **critical scope creep risk**:

1. **Not in functional requirements**: CP-5 (LangChain Integration) requirements FR-029 through FR-037 do not mention dynamic tool discovery
2. **Not in technical specs**: TECH_SPECS.md Section 7 only describes static skill → tool conversion
3. **Ambiguous implementation**: Multiple valid interpretations exist (see below)
4. **Significant complexity**: Requires API changes, script parsing, metadata extraction, testing
5. **Budget risk**: Would add 15-20 hours to 60-hour v0.1 budget → 25-33% increase
6. **Not blocking validation**: The core progressive disclosure hypothesis can be validated without this feature

### Why It Belongs in v0.2

- **High value**: Aligns with Anthropic's vision of skills bundling executable code
- **Natural extension**: Builds on v0.1 foundation without breaking changes (if designed carefully)
- **User-requested**: Based on SKILL format specification, users expect skills to include scripts
- **Differentiator**: Distinguishes skills-use from simple prompt libraries

---

## Design Decisions Required

### Decision 1: Implementation Approach

#### Option A: Passthrough (Minimal Implementation)

**Description**: The library does not actively discover or create tools from scripts. It only:
- Injects the base directory into skill content
- Returns the processed content as a string
- The agent reads the content, sees mentions of scripts (e.g., "Use scripts/tool.py"), and decides whether to execute them manually

**Pros**:
- Zero additional implementation in skills-use
- Agent has full control over script execution
- No API changes required

**Cons**:
- Scripts are not first-class tools
- Agent must manually parse references and execute scripts
- Doesn't match acceptance scenario (original): "provides the agent with a new tool"
- Less powerful than Anthropic's native implementation

**Recommendation**: ❌ Not recommended—doesn't deliver the expected value

---

#### Option B: Active Discovery (Full Implementation)

**Description**: The library actively discovers scripts in the `scripts/` directory and creates LangChain `StructuredTool` instances for each one.

**Pros**:
- Scripts become first-class tools
- Agent can invoke tools directly without manual execution
- Aligns with Anthropic's skill philosophy
- Delivers clear value over simple prompt libraries

**Cons**:
- Requires API changes (see Decision 2)
- Needs script metadata extraction (see Decision 3)
- Adds complexity: discovery, parsing, tool creation, testing
- Security considerations: script sandboxing, path validation

**Recommendation**: ✅ Recommended—delivers expected value, aligns with vision

---

### Decision 2: API Design

If we choose **Option B (Active Discovery)**, how should the API change?

#### Option B1: Return Tuple

```python
content, discovered_tools = manager.invoke_skill("pdf-processor", "analyze report.pdf")

# content: str (processed SKILL.md)
# discovered_tools: List[StructuredTool] (from scripts/)
```

**Pros**: Explicit, clear separation
**Cons**: Breaking change for v0.1 users

---

#### Option B2: Separate Method

```python
content = manager.invoke_skill("pdf-processor", "analyze report.pdf")
tools = manager.get_skill_tools("pdf-processor")

# Two separate calls
```

**Pros**: Backward compatible, no breaking changes
**Cons**: Two calls required, possible inconsistency if skill changes between calls

---

#### Option B3: Tool Registry Pattern

```python
manager.invoke_skill("pdf-processor", "analyze report.pdf")
# Side effect: Discovered tools are registered globally

tools = manager.get_registered_tools()
# Returns all tools discovered so far
```

**Pros**: No breaking changes, global tool registry
**Cons**: Side effects, harder to reason about, global state management

---

#### Option B4: Event/Callback Pattern

```python
def on_tools_discovered(tools: List[StructuredTool]):
    print(f"Discovered {len(tools)} tools!")

manager.on_tool_discovery(on_tools_discovered)
content = manager.invoke_skill("pdf-processor", "analyze report.pdf")
# Callback invoked if tools discovered
```

**Pros**: Flexible, backward compatible
**Cons**: More complex, callback management overhead

---

**Recommended**: **Option B2 (Separate Method)** or **Option B1 (Return Tuple)**
- B2 for backward compatibility
- B1 for simplicity and explicitness

---

### Decision 3: Script Metadata Extraction

How does the library determine the **name** and **description** of each tool from a script file?

#### Option 3A: Filename Convention

```
scripts/
├── extract_text_from_pdf.py  → tool name: "extract_text_from_pdf"
├── compress_pdf.py           → tool name: "compress_pdf"
└── merge_pdfs.sh             → tool name: "merge_pdfs"
```

**Description**: Inferred from filename
**Pros**: Simple, no parsing required
**Cons**: No descriptions, limited metadata

---

#### Option 3B: Docstring Parsing

```python
# scripts/extract_text.py
def main(pdf_path: str) -> str:
    """
    Extract text content from a PDF file.

    Args:
        pdf_path: Path to the PDF file to process

    Returns:
        Extracted text content
    """
    ...
```

**Metadata extraction**:
- Tool name: `extract_text` (from filename)
- Description: First line of docstring ("Extract text content from a PDF file.")
- Parameters: Parsed from Args section (optional)

**Pros**: Rich metadata, standard Python convention
**Cons**: Requires parsing, only works for Python

---

#### Option 3C: Manifest File

```yaml
# scripts/manifest.yml
tools:
  - name: extract_text_from_pdf
    script: extract_text.py
    description: Extract text content from a PDF file
    parameters:
      - name: pdf_path
        type: string
        description: Path to the PDF file

  - name: compress_pdf
    script: compress.py
    description: Compress a PDF file to reduce size
    parameters:
      - name: input_path
        type: string
      - name: output_path
        type: string
```

**Pros**: Language-agnostic, explicit metadata, full control
**Cons**: Extra file to maintain, manual metadata entry

---

**Recommended**: **Hybrid Approach**
1. **Filename convention** (always)
2. **Docstring parsing** (Python scripts only, optional)
3. **Manifest file** (optional override)

Priority: Manifest > Docstring > Filename

---

### Decision 4: Script Execution Model

How are scripts executed when tools are invoked?

#### Option 4A: Subprocess Execution

```python
result = subprocess.run(
    ["python", script_path, arg1, arg2],
    capture_output=True,
    text=True,
    timeout=30
)
```

**Pros**: Sandboxed, timeout support, language-agnostic
**Cons**: Slower, no state sharing

---

#### Option 4B: In-Process Execution (Python only)

```python
import importlib.util
spec = importlib.util.spec_from_file_location("tool", script_path)
module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(module)
result = module.main(arg1, arg2)
```

**Pros**: Faster, can share state
**Cons**: Security risk, Python-only

---

**Recommended**: **Option 4A (Subprocess)** for v0.2—security and language-agnosticism are more important than performance

---

## Proposed Functional Requirements

### FR-201: Script Discovery

- **FR-201.1**: System MUST scan the `scripts/` directory within a skill directory when a skill is loaded
- **FR-201.2**: System MUST discover all executable scripts (`.py`, `.sh`, `.js`) in the `scripts/` directory
- **FR-201.3**: System MUST ignore non-executable files and subdirectories within `scripts/`
- **FR-201.4**: System MUST handle missing `scripts/` directory gracefully (no errors, zero tools discovered)

### FR-202: Tool Metadata Extraction

- **FR-202.1**: System MUST extract tool name from script filename (e.g., `extract_text.py` → `extract_text`)
- **FR-202.2**: System SHOULD extract tool description from Python docstrings (first line)
- **FR-202.3**: System MAY support `scripts/manifest.yml` for explicit metadata override
- **FR-202.4**: System MUST provide a default description if no metadata is found (e.g., "Execute {script_name}")

### FR-203: LangChain Tool Creation

- **FR-203.1**: System MUST create a `StructuredTool` for each discovered script
- **FR-203.2**: Tool name MUST be unique within a skill (skill_name + script_name)
- **FR-203.3**: Tool invocation MUST execute the script via subprocess with timeout (30s default)
- **FR-203.4**: Tool MUST capture stdout and return it as the tool result
- **FR-203.5**: Tool MUST capture stderr and log errors appropriately

### FR-204: API Extension

- **FR-204.1**: System MUST provide `manager.get_skill_tools(skill_name)` method returning `List[StructuredTool]`
- **FR-204.2**: System MUST NOT break v0.1 API (`invoke_skill` returns string)
- **FR-204.3**: System SHOULD cache discovered tools to avoid re-scanning on each call

### FR-205: Security

- **FR-205.1**: System MUST validate script paths stay within skill directory (path traversal prevention)
- **FR-205.2**: System MUST execute scripts with configurable timeout (prevent hanging)
- **FR-205.3**: System MUST NOT execute scripts with setuid/setgid permissions
- **FR-205.4**: System SHOULD log all script executions for auditing

---

## Acceptance Criteria

### AC-1: Discovery

**Given** a skill directory with structure:
```
my-skill/
├── SKILL.md
└── scripts/
    ├── tool1.py
    ├── tool2.sh
    └── helper.txt  (ignored)
```

**When** `manager.get_skill_tools("my-skill")` is called
**Then** 2 tools are returned (tool1, tool2), and helper.txt is ignored

---

### AC-2: Metadata Extraction

**Given** a Python script with docstring:
```python
def main(input_path: str):
    """Process a file and return results."""
    ...
```

**When** the tool is created
**Then** the tool description is "Process a file and return results."

---

### AC-3: Tool Invocation

**Given** a skill tool created from `scripts/greet.py`:
```python
import sys
print(f"Hello, {sys.argv[1]}!")
```

**When** the tool is invoked with arguments `"World"`
**Then** the tool returns `"Hello, World!"`

---

### AC-4: Error Handling

**Given** a script that fails with exit code 1
**When** the tool is invoked
**Then** the tool raises `SkillInvocationError` with stderr content

---

### AC-5: Backward Compatibility

**Given** v0.1 code using `invoke_skill()`
**When** v0.2 is installed
**Then** the code continues to work without modification

---

## Estimated Effort

| Task | Hours |
|------|-------|
| Design finalization (decisions 1-4) | 2 |
| Script discovery implementation (FR-201) | 3 |
| Metadata extraction (FR-202) | 4 |
| Tool creation (FR-203) | 4 |
| API extension (FR-204) | 2 |
| Security implementation (FR-205) | 3 |
| Testing (unit + integration) | 5 |
| Documentation updates | 2 |
| **Total** | **25 hours** |

**Buffer (20%)**: 5 hours
**Total with buffer**: **30 hours**

**Timeline**: 1 week full-time or 2 weeks half-time

---

## Testing Strategy

### Unit Tests

- `test_script_discovery.py`: Test scanning scripts/ directory
- `test_metadata_extraction.py`: Test docstring parsing, manifest parsing
- `test_tool_creation.py`: Test StructuredTool creation from scripts
- `test_script_execution.py`: Test subprocess execution, timeout handling

### Integration Tests

- `test_langchain_dynamic_tools.py`: End-to-end test with LangChain agent discovering and invoking script tools
- `test_skill_with_scripts.py`: Test full skill lifecycle (discovery → invocation → tool usage)

### Example Skills for Testing

Create test fixtures:
- `fixtures/skills/calculator/` with `scripts/add.py`, `scripts/multiply.py`
- `fixtures/skills/file-processor/` with `scripts/read_file.py`, `scripts/write_file.py`

---

## Success Metrics

- ✅ All tests passing (85%+ coverage for new code)
- ✅ No breaking changes to v0.1 API
- ✅ Agent can discover and invoke script tools in end-to-end test
- ✅ Documentation updated with examples
- ✅ Security validation: path traversal prevented, timeouts enforced

---

## Open Questions

1. **Async support**: Should we implement `async` execution for scripts in v0.2, or defer to later?
2. **Parameter schemas**: Should we parse function signatures to create typed tool schemas, or use generic `str` input?
3. **Multi-language support**: Should v0.2 support JavaScript (Node.js) and Shell scripts, or Python-only initially?
4. **Caching**: Should discovered tools be cached, or re-scanned on each `get_skill_tools()` call?

---

## References

- Original acceptance scenario (removed from v0.1): `specs/001-mvp-langchain-core/spec.md` (User Story 4, Scenario #4)
- SKILL format specification: `.docs/SKILL format specification.md` (lines 86-88)
- MVP plan: `.docs/MVP_VERTICAL_SLICE_PLAN.md`
- Technical specs: `.docs/TECH_SPECS.md` (Section 7: LangChain Integration)

---

## Next Steps

1. **Review this document** with stakeholders
2. **Make design decisions** (Decisions 1-4)
3. **Create detailed implementation plan** for v0.2 sprint
4. **Update v0.2 milestone** with tasks from effort estimate
5. **Begin implementation** after v0.1.0 is released and validated

---

**Document Status**: Draft
**Last Updated**: 2025-11-03
**Author**: Massimo Olivieri
